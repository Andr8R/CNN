{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# August 28 - Training on 9M event dataset metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default imports\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "# Add the path to the parent directory to augment search for module\n",
    "par_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if par_dir not in sys.path:\n",
    "    sys.path.append(par_dir)\n",
    "    \n",
    "# Import the custom plotting module\n",
    "from plot_utils import plot_utils\n",
    "import random\n",
    "import torch\n",
    "from plot_utils import notebook_utils_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch - Extract the values from the saved .npz array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_npz_path = \"/home/akajal/WatChMaL/VAE/dumps/20190828_235444/test_valid_iteration_metrics.npz\"\n",
    "dump_npz_arr = np.load(dump_npz_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(dump_npz_arr.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, recon_loss, kl_loss = dump_npz_arr[\"indices\"], dump_npz_arr[\"recon_loss\"], dump_npz_arr[\"kl_loss\"]\n",
    "print(indices.shape)\n",
    "print(recon_loss.shape)\n",
    "print(kl_loss.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for duplicates in the npz list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(indices)\n",
    "print(np.where(counts > 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recon_loss[:10])\n",
    "print(kl_loss[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(indices[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the metric dumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the indices exist in the dumps for all the latent dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = [16, 32, 64, 128, 256]\n",
    "dumps = [\"20190829_010238\", \"20190829_010252\", \"20190829_010339\", \"20190829_010405\", \"20190829_010431\"]\n",
    "\n",
    "# First check that all the indices from the test validation set exist in all the dumps\n",
    "\n",
    "ldump_idx_arr = None\n",
    "\n",
    "# Iterate over the dumps and check the indices\n",
    "for latent_dim, dump in zip(latent_dims, dumps):\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(\"Reading metrics from VAE with {0} latent dimensions :\".format(latent_dim))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    \n",
    "    dump_npz_path = \"/home/akajal/WatChMaL/VAE/dumps/{0}/test_valid_iteration_metrics.npz\".format(dump)\n",
    "    dump_npz_arr = np.load(dump_npz_path)\n",
    "    \n",
    "    dump_indices = np.sort(dump_npz_arr[\"indices\"])\n",
    "    \n",
    "    if ldump_idx_arr is not None:\n",
    "        if not np.array_equal(dump_indices, ldump_idx_arr):\n",
    "            print(\"Index array for latent dims {0} not equal to all the other.\".format(latent_dim))\n",
    "        else:\n",
    "            print(\"Index array equal to the first index array\")\n",
    "    else:\n",
    "        ldump_idx_arr = dump_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the first level metrics from the dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_dims = [16, 32, 64, 128, 256]\n",
    "dumps = [\"20190829_010238\", \"20190829_010252\", \"20190829_010339\", \"20190829_010405\", \"20190829_010431\"]\n",
    "\n",
    "# Metrics for plotting\n",
    "recon_loss_values, kl_loss_values = [], []\n",
    "recon_std_values, kl_std_values, recon_stderr_values, kl_stderr_values = [], [], [], []\n",
    "\n",
    "# Iterate over the dumps and check the indices\n",
    "for latent_dim, dump in zip(latent_dims, dumps):\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(\"Printing metrics for VAE with {0} latent dimensions :\".format(latent_dim))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    \n",
    "    dump_npz_path = \"/home/akajal/WatChMaL/VAE/dumps/{0}/test_valid_iteration_metrics.npz\".format(dump)\n",
    "    npz_arr = np.load(dump_npz_path)\n",
    "    \n",
    "    dump_recon_loss, dump_kl_loss = npz_arr[\"recon_loss\"], npz_arr[\"kl_loss\"]\n",
    "    \n",
    "    mean_recon_loss, std_recon_loss = np.mean(dump_recon_loss), np.std(dump_recon_loss)\n",
    "    stderr_recon_loss = std_recon_loss/math.sqrt(dump_recon_loss.shape[0])\n",
    "    \n",
    "    recon_loss_values.append(mean_recon_loss)\n",
    "    recon_std_values.append(std_recon_loss)\n",
    "    recon_stderr_values.append(stderr_recon_loss)\n",
    "    \n",
    "    mean_kl_loss, std_kl_loss = np.mean(dump_kl_loss), np.std(dump_kl_loss)\n",
    "    stderr_kl_loss = std_kl_loss/math.sqrt(dump_kl_loss.shape[0])\n",
    "    \n",
    "    kl_loss_values.append(mean_kl_loss)\n",
    "    kl_std_values.append(std_kl_loss)\n",
    "    kl_stderr_values.append(stderr_kl_loss)\n",
    "    \n",
    "    print(\"Recon Loss metrics\")\n",
    "    print(\"Mean Recon loss : {0}\".format(mean_recon_loss))\n",
    "    print(\"Std Recon loss : {0}\".format(std_recon_loss))\n",
    "    print(\"Stderr Recon loss : {0}\\n\".format(stderr_recon_loss))\n",
    "    \n",
    "    print(\"KL Loss metrics\")\n",
    "    print(\"Mean KL loss : {0}\".format(mean_kl_loss))\n",
    "    print(\"Std KL loss : {0}\".format(std_kl_loss))\n",
    "    print(\"Stderr KL loss : {0}\".format(stderr_kl_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the per-sample measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the metrics for the training subset\n",
    "\n",
    "# Initialize the plot\n",
    "fig, ax1 = plt.subplots(figsize=(16,9))\n",
    "\n",
    "# Set the x-axes ticks for the plot\n",
    "ax1.set_xticks(latent_dims)\n",
    "\n",
    "# Use the same x-axis to plot the KL loss\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the MSE values collected above\n",
    "ax1.errorbar(latent_dims, recon_loss_values, yerr=recon_stderr_values, linestyle='dashed', marker='o',\n",
    "             color=\"blue\", label=\"Mean MSE loss (per sample)\")\n",
    "\n",
    "# Plot the KL values collected above\n",
    "ax2.errorbar(latent_dims, kl_loss_values, yerr=kl_stderr_values, linestyle='dashed', marker='o',\n",
    "             color=\"red\", label=\"Mean KL loss (per sample)\")\n",
    "\n",
    "# Setup plot characteristics\n",
    "ax1.tick_params(axis=\"x\", labelsize=30)\n",
    "ax1.set_xlabel(\"Number of latent dimensions\", fontsize=30)\n",
    "ax1.set_xscale(\"log\", basex=2)\n",
    "\n",
    "ax1.set_ylabel(\"Recon (MSE) loss\", fontsize=30, color=\"blue\")\n",
    "ax1.tick_params(axis=\"y\", labelsize=30, colors=\"blue\")\n",
    "\n",
    "ax2.set_ylabel(\"KL loss\", fontsize=30, color=\"red\")\n",
    "ax2.tick_params(axis=\"y\", labelsize=30, colors=\"red\")\n",
    "\n",
    "plt.margins(0.2)\n",
    "ax1.grid(True)\n",
    "ax2.grid(True)\n",
    "plt.title(\"Per-sample Loss vs Latent dimensions (test)\", fontsize=30)\n",
    "\n",
    "lgd = fig.legend(prop={\"size\":25}, loc='center', bbox_to_anchor=(0.5, 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above is a per-sample measure. How about per-batch ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for plotting\n",
    "recon_loss, kl_loss = [], []\n",
    "recon_std_values, kl_std_values, recon_stderr_values, kl_stderr_values = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vae_test_metrics(run_id):\n",
    "    \n",
    "    # Using the absolute path\n",
    "    dump_dir = \"/home/akajal/WatChMaL/VAE/dumps/\" + run_id + \"/\"\n",
    "    train_val_log = dump_dir + \"test_validation_log.csv\"\n",
    "    \n",
    "    # Print the average metrics on the training subset\n",
    "    log_df = pd.read_csv(train_val_log)\n",
    "            \n",
    "    # Extract the loss values from the csv file\n",
    "    loss_values = log_df[\"loss\"].values\n",
    "    mse_loss_values  = log_df[\"recon_loss\"].values\n",
    "    kl_loss_values = log_df[\"kl_loss\"].values\n",
    "    \n",
    "    test_total_loss = np.mean(loss_values)\n",
    "    test_mse_loss, test_kl_loss  = np.mean(mse_loss_values), np.mean(kl_loss_values)\n",
    "    test_mse_std, test_kl_std = np.std(mse_loss_values), np.std(kl_loss_values)\n",
    "    test_mse_stderr, test_kl_stderr = test_mse_std/math.sqrt(mse_loss_values.shape[0]), test_kl_std/math.sqrt(kl_loss_values.shape[0])  \n",
    "    \n",
    "    recon_loss.append(test_mse_loss)\n",
    "    kl_loss.append(test_kl_loss)\n",
    "    \n",
    "    recon_stderr_values.append(test_mse_stderr)\n",
    "    kl_stderr_values.append(test_kl_stderr)\n",
    "    \n",
    "    # Print out the average values\n",
    "    print(\"Recon Loss metrics\")\n",
    "    print(\"Mean Recon loss : {0}\".format(test_mse_loss))\n",
    "    print(\"Std Recon loss : {0}\".format(std_recon_loss))\n",
    "    print(\"Stderr Recon loss : {0}\\n\".format(stderr_recon_loss))\n",
    "    \n",
    "    print(\"KL Loss metrics\")\n",
    "    print(\"Mean KL loss : {0}\".format(mean_kl_loss))\n",
    "    print(\"Std KL loss : {0}\".format(std_kl_loss))\n",
    "    print(\"Stderr KL loss : {0}\".format(stderr_kl_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over the dumps and check the indices\n",
    "for latent_dim, dump in zip(latent_dims, dumps):\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(\"Printing metrics for VAE with {0} latent dimensions :\".format(latent_dim))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    \n",
    "    print_vae_test_metrics(dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the per-batch measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the metrics for the training subset\n",
    "\n",
    "# Initialize the plot\n",
    "fig, ax1 = plt.subplots(figsize=(16,9))\n",
    "\n",
    "# Set the x-axes ticks for the plot\n",
    "ax1.set_xticks(latent_dims)\n",
    "\n",
    "# Use the same x-axis to plot the KL loss\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the MSE values collected above\n",
    "ax1.errorbar(latent_dims, recon_loss, yerr=recon_stderr_values, linestyle='dashed', marker='o',\n",
    "             color=\"blue\", label=\"Mean MSE loss (per batch sample)\")\n",
    "\n",
    "# Plot the KL values collected above\n",
    "ax2.errorbar(latent_dims, kl_loss, yerr=kl_stderr_values, linestyle='dashed', marker='o',\n",
    "             color=\"red\", label=\"Mean KL loss (per batch sample)\")\n",
    "\n",
    "# Setup plot characteristics\n",
    "ax1.tick_params(axis=\"x\", labelsize=30)\n",
    "ax1.set_xlabel(\"Number of latent dimensions\", fontsize=30)\n",
    "ax1.set_xscale(\"log\", basex=2)\n",
    "\n",
    "ax1.set_ylabel(\"Recon (MSE) loss\", fontsize=30, color=\"blue\")\n",
    "ax1.tick_params(axis=\"y\", labelsize=30, colors=\"blue\")\n",
    "\n",
    "ax2.set_ylabel(\"KL loss\", fontsize=30, color=\"red\")\n",
    "ax2.tick_params(axis=\"y\", labelsize=30, colors=\"red\")\n",
    "\n",
    "plt.margins(0.2)\n",
    "ax1.grid(True)\n",
    "ax2.grid(True)\n",
    "plt.title(\"Per-batch-sample Loss vs Latent dimensions (test)\", fontsize=30)\n",
    "\n",
    "lgd = fig.legend(prop={\"size\":25}, loc='center', bbox_to_anchor=(0.5, 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
