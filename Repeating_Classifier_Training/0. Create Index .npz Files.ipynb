{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Index Files\n",
    "### In this notebook, we create the necessary index files for rearranging FiTQun output in the same order as the h5 test set, which is done in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "from collections import Counter\n",
    "from progressbar import *\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Add the path to the parent directory to augment search for module\n",
    "par_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "if par_dir not in sys.path:\n",
    "    sys.path.append(par_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping the ordinal labels to particle types \n",
    "LABEL_DICT = {0:\"gamma\", 1:\"e\", 2:\"mu\"}\n",
    "\n",
    "# Fix the colour scheme for each particle type\n",
    "COLOR_DICT = {\"gamma\":\"red\", \"e\":\"blue\", \"mu\":\"green\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputting files to /home/cmacdonald/CNN/Index_Storage\n"
     ]
    }
   ],
   "source": [
    "output_path = os.path.join(os.getcwd(),'Index_Storage')\n",
    "print(\"Outputting files to \" + output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load original test dataset (load full h5 and apply test indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original h5 file info\n",
    "\n",
    "# Import test events from h5 file\n",
    "filtered_index = \"/fast_scratch/WatChMaL/data/IWCD_fulltank_300_pe_idxs.npz\"\n",
    "filtered_indices = np.load(filtered_index, allow_pickle=True)\n",
    "test_filtered_indices = filtered_indices['test_idxs']\n",
    "\n",
    "original_data_path = \"/data/WatChMaL/data/IWCDmPMT_4pi_fulltank_9M.h5\"\n",
    "f = h5py.File(original_data_path, \"r\")\n",
    "\n",
    "hdf5_event_data = (f[\"event_data\"])\n",
    "original_eventdata = np.memmap(original_data_path, mode=\"r\", shape=hdf5_event_data.shape,\n",
    "                                    offset=hdf5_event_data.id.get_offset(), dtype=hdf5_event_data.dtype)\n",
    "\n",
    "original_eventids = np.array(f['event_ids'])\n",
    "original_rootfiles = np.array(f['root_files'])\n",
    "original_energies = np.array(f['energies'])\n",
    "original_positions = np.array(f['positions'])\n",
    "original_angles = np.array(f['angles'])\n",
    "original_labels = np.array(f['labels'])\n",
    "#filtered_eventdata = original_eventdata[test_filtered_indices]\n",
    "filtered_eventids = original_eventids[test_filtered_indices]\n",
    "filtered_rootfiles = original_rootfiles[test_filtered_indices]\n",
    "filtered_energies = original_energies[test_filtered_indices]\n",
    "filtered_positions = original_positions[test_filtered_indices]\n",
    "filtered_angles = original_angles[test_filtered_indices]\n",
    "filtered_labels = original_labels[test_filtered_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find indices in h5 test set pointing to rootfile/eventid pairs for which FiTQun produced no output\n",
    "This is the slowest step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_files = np.load('/data/WatChMaL/data/missing_files.npz',allow_pickle=True)['arr_0']\n",
    "failed_eventids = np.load('/data/WatChMaL/data/missing_eventids.npz',allow_pickle=True)['arr_0']\n",
    "assert failed_files.shape[0] == failed_eventids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Progress: 100% [0000000000000000000000000000000000000000] Time: 0:12:25\n"
     ]
    }
   ],
   "source": [
    "fq_failed_idxs = np.array([])\n",
    "pbar = ProgressBar(widgets=['Mapping Progress: ', Percentage(), ' ', Bar(marker='0',left='[',right=']'),\n",
    "           ' ', ETA()], maxval=len(failed_files))\n",
    "pbar.start()\n",
    "for i in range(len(failed_files)):\n",
    "    matching_file_idxs = np.where(filtered_rootfiles == failed_files[i])[0]\n",
    "    ind1 = np.where(filtered_eventids[matching_file_idxs] == failed_eventids[i])[0]\n",
    "    fq_failed_idxs = np.append(fq_failed_idxs, matching_file_idxs[ind1] )      \n",
    "    pbar.update(i)\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(os.path.join(output_path,'fq_failed_idxs'),failed_indices_pointing_to_h5_test_set=fq_failed_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that our indices point to the right files in the h5 test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! We have identified the indices in the h5 test set pointing to files that FiTQun failed on\n"
     ]
    }
   ],
   "source": [
    "# check that we have the right indices\n",
    "fq_failed_idxs = np.load(os.path.join(output_path,'fq_failed_idxs.npz'), allow_pickle = True)['failed_indices_pointing_to_h5_test_set']\n",
    "for i in range(len(failed_files)):\n",
    "    assert failed_files[i] == filtered_rootfiles[int(fq_failed_idxs[i])]\n",
    "    assert failed_eventids[i] == filtered_eventids[int(fq_failed_idxs[i])]\n",
    "print(\"Success! We have identified the indices in the h5 test set pointing to files that FiTQun failed on\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out the events that FiTQun failed on from the h5 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "  \n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "  \"\"\"\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "sfiltered_eventids = np.delete(filtered_eventids, fq_failed_idxs)\n",
    "sfiltered_rootfiles = np.delete(filtered_rootfiles , fq_failed_idxs)\n",
    "sfiltered_energies = np.delete(filtered_energies, fq_failed_idxs)\n",
    "sfiltered_positions = np.delete(filtered_positions, fq_failed_idxs)\n",
    "sfiltered_angles = np.delete(filtered_angles, fq_failed_idxs,0)\n",
    "sfiltered_labels = np.delete(filtered_labels, fq_failed_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the fiTQun output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for fiTQun results\n",
    "fiTQun_e_path = \"/fast_scratch/WatChMaL/data/IWCDmPMT_4pi_fulltank_fiTQun_e-.npz\"\n",
    "fiTQun_mu_path = \"/fast_scratch/WatChMaL/data/IWCDmPMT_4pi_fulltank_fiTQun_mu-.npz\"\n",
    "fiTQun_gamma_path = \"/fast_scratch/WatChMaL/data/IWCDmPMT_4pi_fulltank_fiTQun_gamma.npz\"\n",
    "\n",
    "# Load fiTQun results\n",
    "f_e = np.load(fiTQun_e_path, allow_pickle=True)\n",
    "f_mu = np.load(fiTQun_mu_path, allow_pickle=True)\n",
    "f_gamma = np.load(fiTQun_gamma_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make some dictionaries to use in finding indices to reorder FiTQun data in same order as h5 test set. \n",
    "Note that, for some reason, the fitqun eventids are one-indexed, while the h5 eventids are zero-indexed. Since we want to use h5 rootfile/eventid pairs as keys, we insert the fitqun eventids -1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fq_e_filenames = f_e['filename']\n",
    "fq_m_filenames = f_mu['filename']\n",
    "fq_g_filenames = f_gamma['filename']\n",
    "fq_e_eventids = f_e['eventid']\n",
    "fq_m_eventids = f_mu['eventid']\n",
    "fq_g_eventids = f_gamma['eventid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Electron Event Dictionary: 100% [0000000000000000000000] Time: 0:00:06\n",
      "Creating Gamma Event Dictionary: 100% [0000000000000000000000000] Time: 0:00:06\n",
      "Creating Muon Event Dictionary: 100% [00000000000000000000000000] Time: 0:00:06\n"
     ]
    }
   ],
   "source": [
    "fq_e_dict = {}\n",
    "pbar = ProgressBar(widgets=['Creating Electron Event Dictionary: ', Percentage(), ' ', Bar(marker='0',left='[',right=']'),\n",
    "           ' ', ETA()], maxval=len(f_e['eventid']))\n",
    "pbar.start()\n",
    "for i in range(len(f_e['eventid'])):\n",
    "    fq_e_dict[(re.sub('_fiTQun','',fq_e_filenames[i].split('/')[-1]), fq_e_eventids[i]-1)] = i\n",
    "    pbar.update(i)\n",
    "pbar.finish()\n",
    "\n",
    "fq_g_dict = {}\n",
    "pbar = ProgressBar(widgets=['Creating Gamma Event Dictionary: ', Percentage(), ' ', Bar(marker='0',left='[',right=']'),\n",
    "           ' ', ETA()], maxval=len(fq_g_filenames))\n",
    "pbar.start()\n",
    "for i in range(len(fq_g_filenames)):\n",
    "    fq_g_dict[(re.sub('_fiTQun','',fq_g_filenames[i].split('/')[-1]), fq_g_eventids[i]-1)] = i\n",
    "    pbar.update(i)\n",
    "pbar.finish()\n",
    "\n",
    "fq_m_dict = {}\n",
    "pbar = ProgressBar(widgets=['Creating Muon Event Dictionary: ', Percentage(), ' ', Bar(marker='0',left='[',right=']'),\n",
    "           ' ', ETA()], maxval=len(fq_m_filenames))\n",
    "pbar.start()\n",
    "for i in range(len(fq_m_filenames)):\n",
    "    fq_m_dict[(re.sub('_fiTQun','',fq_m_filenames[i].split('/')[-1]), fq_m_eventids[i]-1)] = i\n",
    "    pbar.update(i)\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find mapping indices\n",
    "The output fq_mapping_indices is an array such that fq_mapping_indices \\[i\\] is the index of the fitqun output in its e-, gamma, or mu output arrays with the same root file and event id as the ith event in the h5 test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping Progress: 100% [0000000000000000000000000000000000000000] Time: 0:00:29\n"
     ]
    }
   ],
   "source": [
    "fq_mapping_indices = np.zeros(len(sfiltered_rootfiles))\n",
    "pbar = ProgressBar(widgets=['Mapping Progress: ', Percentage(), ' ', Bar(marker='0',left='[',right=']'),\n",
    "           ' ', ETA()], maxval=len(sfiltered_rootfiles))\n",
    "pbar.start()\n",
    "for i in range(len(sfiltered_rootfiles)):\n",
    "    if sfiltered_labels[i]==0:\n",
    "        fq_mapping_indices[i] = fq_g_dict[(sfiltered_rootfiles[i].split('/')[-1], sfiltered_eventids[i])]\n",
    "    elif sfiltered_labels[i]==1:\n",
    "        fq_mapping_indices[i] = fq_e_dict[(sfiltered_rootfiles[i].split('/')[-1], sfiltered_eventids[i])]\n",
    "    elif sfiltered_labels[i]==2:\n",
    "        fq_mapping_indices[i] = fq_m_dict[(sfiltered_rootfiles[i].split('/')[-1], sfiltered_eventids[i])]\n",
    "    pbar.update(i)\n",
    "pbar.finish()\n",
    "fq_mapping_indices = np.int32(fq_mapping_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that we didn't make any mistakes in the ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verification Progress: 100% [00000000000000000000000000000000000] Time: 0:00:36\n"
     ]
    }
   ],
   "source": [
    "pbar = ProgressBar(widgets=['Verification Progress: ', Percentage(), ' ', Bar(marker='0',left='[',right=']'),\n",
    "           ' ', ETA()], maxval=len(sfiltered_rootfiles))\n",
    "pbar.start()\n",
    "for i in range(len(sfiltered_rootfiles)):\n",
    "    if sfiltered_labels[i]==0:\n",
    "        assert re.sub('_fiTQun','',fq_g_filenames[fq_mapping_indices[i]].split('/')[-1]) == sfiltered_rootfiles[i].split('/')[-1]\n",
    "        assert fq_g_eventids[fq_mapping_indices[i]] -1 == sfiltered_eventids[i]\n",
    "    elif sfiltered_labels[i]==1:\n",
    "        assert re.sub('_fiTQun','',fq_e_filenames[fq_mapping_indices[i]].split('/')[-1]) == sfiltered_rootfiles[i].split('/')[-1]\n",
    "        assert fq_e_eventids[fq_mapping_indices[i]] -1 == sfiltered_eventids[i]\n",
    "    elif sfiltered_labels[i]==2:\n",
    "        assert re.sub('_fiTQun','',fq_m_filenames[fq_mapping_indices[i]].split('/')[-1]) == sfiltered_rootfiles[i].split('/')[-1]\n",
    "        assert fq_m_eventids[fq_mapping_indices[i]] -1 == sfiltered_eventids[i]\n",
    "    else:\n",
    "        assert False\n",
    "    pbar.update(i)\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(os.path.join(output_path, 'fq_mapping_idxs.npz'),fq_mapping_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit31338303183c4c20baadc3c2a7d305e6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
